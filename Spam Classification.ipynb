{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('emails.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT SPAM  :  4360\n",
      "SPAM  :  1368\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = ['NOT SPAM','SPAM']\n",
    "labels = df['spam']\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "values = list(zip(unique, counts))\n",
    "plt.bar(classes,counts)\n",
    "for i in values:\n",
    "    print(classes[i[0]],' : ',i[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Emails from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Subject: boss ' s day  hey everyone ,  i know you may not be aware that boss ' s day oct . 16 , 2000  we will celebrate as a group in the staff meeting on oct . 19 th ,  with the big boss vince kaminski and all the others , however ,  if you would like to do something special for our boss , please  inform me whereby i can make arrangements .  thanks  kevin moore\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 : Subject: reply to your email / ignore my voicemail  please respond to vince :  thanks for that . i just wanted to get a sense from you who the right people  are and how i can establish effective contact . when he went on to different  responsibilities , john goodpasture suggested i get the dialog going with the  right commercial people in enron . i will be in your neighborhood in the 200  pm time range and will give you a quick call . that will conserve your  valuable time and hopefully get me in touch with the right people . i am  reading this after your voicemail , so this supersedes that .  dale  - - - - - original message - - - - -  from : vince . j . kaminski @ enron . com [ mailto : vince . j . kaminski @ enron . com ]  sent : tuesday , may 01 , 2001 6 : 03 am  to : dale . nesbitt @ marketpointinc . com  cc : vince . j . kaminski @ enron . com  subject : re : get together this coming tuesday ?  dale ,  i can reserve 2 to 2 : 30 time slot but there is really not much that  i can tell you at this point .  the commercial groups are still interested and are moving  towards the test of the package . as soon as they will decide  to move ahead , we ( research ) shall be involved , helping to evaluate the  product . as i have said , we are not the  decision makers in this case .  i think that we should allow simply the process to run its course .  vince  \" dale m . nesbitt \" on 04 / 30 / 2001 05 : 59 : 30  pm  please respond to  to :  cc :  subject : re : get together this coming tuesday ?  vince :  i will call tomorrow in the morning . lunch or right after lunch would be  great . how would 100 pm work for you ?  dale  - - - - - original message - - - - -  from : vince . j . kaminski @ enron . com [ mailto : vince . j . kaminski @ enron . com ]  sent : monday , april 30 , 2001 3 : 07 pm  to : dale . nesbitt @ marketpointinc . com  cc : kimberly . watson @ enron . com ; vince . j . kaminski @ enron . com  subject : re : get together this coming tuesday ?  dale ,  please , call me on tuesday . my morning schedule is full but i am open in  the afternoon .  vince  \" dale m . nesbitt \" on 04 / 30 / 2001 01 : 51 : 21  am  please respond to  to : \" vincent kaminski \" , \" kimberly s . watson \"  cc :  subject : get together this coming tuesday ?  vince / kim :  i am flying to houston tonight and wondered if it would fit one or both of  your schedules to get together this coming tuesday sometime for 1 / 2 hour or  so . i really want to reinitiate the conversations marketpoint was having  with john goodpasture and you , and he said either or both of you were the  right people to continue after his responsibility shift . john was quite  positive about the idea of enron acquiring marketpoint narg through  license ,  and he implied that one or both of you would be carrying the ball in that  direction after he handed it to you .  would this coming tuesday morning at 930 am be a good time for you guys ?  if  so , please give me an email shout at the above address or leave a message  on  my voicemail at ( 650 ) 218 - 3069 . i think you will be truly impressed with  the  scope and progress we have been able to make with both the short run narg  and the long run narg in which you were interested ( not to mention our  power  model ) . the progress is noticeable since you saw it . both long and short  term narg are having quite an impact on a number of gas decisions at the  moment ranging from venezuelan lng , north american lng import terminals and  term , gas basis calculations , trading support , power plant development ,  gas - to - power price spreads in key markets , veracity of heat rate trades ,  bank financings , storage field evaluation , and which new pipelines we can  expect to see enter and which are dogs .  i really hope we can fit it in and get our discussions moving in a mutually  productive direction again . i think narg can help you become even more  successful , and i look forward to working with you .  we have a new office address and new phone number as well . ( we move in may  1 . )  altos management partners  95 main street , suite 10  los altos , ca 94022  ( 650 ) 948 - 8830 voice  ( 650 ) 948 - 8850 fax  ( 650 ) 218 - 3069 cellular  give the phones a week or so to get \" debugged \" and then switch over .  dale\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3 : Subject: interview for japan office  darren ,  tanya and i had a telephone interview for yumi . i do not know what  kind of position you would offer her .  if you intended to let her do the work on quantitative modeling , her knowledge  in math seems very sallow . she is working on a math degree on stochastic  process , but she can not explain what ito ' lemma is . we also asked questions  about volatility of a basket , value at risk , etc . she did not have a clear  answer .  if you intended to let her to be a junior trader , she might be ok . it seems  she has  some experience of financial market , but i think you are much more qualified  to  probe her than i do in this aspect .  keep in touch ,  best regards  zimin  from : darren delage @ enron on 01 / 12 / 2001 11 : 59 am ze 9  to : \" mm 21 yumi \"  cc : zimin lu / hou / ect @ ect  subject : re : next tuesday  good afternoon imokawa - san ,  we would like to invite you to have a brief dialogue with some members of our  research team . they would like to ask you to briefly expound on your  mathematical studies . if you could please contact them next wednesday at  7 : 50 am ( it should be 4 : 50 pm houston time , tuesday ) . the conversation should  take no more than 20 minutes of your time , and will enable us to get a more  enhanced understanding of your quantitative abilities .  zimin lu , director of research , can be reached at 713 - 853 - 6388  to dial from japan , 0061 - 1 - 713 - 853 - 6388  if you could please send zimin a copy of your resume before the interview ,  that would be much appreciated . you can call the above number to obtain the  appropriate fax number .  i will be in touch with you shortly thereafter .  sincerely ,  darren  \" mm 21 yumi \"  01 / 11 / 2001 08 : 35 pm  to :  cc :  subject : thank you  darren , thank you for cordinating everything .  i understand it takes time , this is  only the first week of the year in japan , and i do not like to  push you much . normally , i have long meetings every thursday .  for other dates , i make best effort to fit the schedule for  your convenience , including early morning or late evening .  i am looking forward to seeing you sometime soon .  sincerely ,  yumi imokawa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4 : Subject: parameter estimation  vince ,  i have put together a parameter estimation model , which is continuation of  tanya ' s model .  the estimation process is more consistent now . attached are the model and a  brief  write - up of the methods . if you see any problem / ways to improve it , please  let me know .  best ,  alex\n",
      "----------------------------------------------------------------------------------------------------\n",
      "5 : Subject: hi  hi shirley & vince :  happy new year ! i am in our bombay offices for a couple of days , so am able  to contact you . for some reason , the modem dialup from my computer hasn ' t  been working . hope everything is going well in houston . anitha & i have good  news . we are having our second baby . it was confirmed after we came to india .  shirley , i have a favor to ask you on this matter . can you make an  appointment for anitha with her doctor ? the details are as follows :  dr . dolar patolia  tel : 713 - 756 - 5500  name : anitha kakani  dates : jan 29 th or 31 st .  times : in order of preferance , after 3 pm , 12 - 3 pm , 10 am - 12 noon .  reason : pregnant with due date of aug 8 th . needs a full checkup .  anitha is having severe nausea , so she is taking rest most of the time .  pallavi and i are enjoing our vacation thoroughly .  thanks and best wishes ,  krishna .  ph . 011 - 91 - 40 - 7114833  ps : please count jan 16 th & 17 th as working days for me .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "6 : Subject: your commissions of $ 5000 per week ! ssva  give me 5 minutes and i will show you how  to turn your computer into a cash machine ! ! !  ( available in the us and canada )  i earned $ 25 , 000 last month - send for my bank statements ! ! !  we are giving away a free , 3 - day vacation to all  folks who ask for more info on how to earn $ 1000  per sale simply by using their computer !  you ' ll earn $ 1000 per sale on a low cost product and  $ 3000 per business contact -  and all marketing is from the internet . free  sophisticated and duplicatable marketing system  ( $ 2500 value ) is given to you gratis - as well as all  training and marketing support ! ! !  we invite you to explore , with no obligation , some  information that could turn around your  income and bring in thousands per week !  if you just want some extra income , or if you are a  seasoned marketer - our paint - by - numbers system with live  support ( up to 10 : 00 pm eastern every day ! ) will turn you  into a pro ! ! !  family - loved product ! ! !  we will have you in profit quick - and with  constant , live free support ! ! ! !  request more free info now -  send an email to : growthmarkets @ excite . com with \" send info \" in the subject line ! !  ( do not click reply ! )  this email conforms to commercial email regulations within the us .  please send any \" remove \" requests to : growthmarkets @ excite . com  - - - -  this sf . net email is sponsored by : thinkgeek  welcome to geek heaven .  http : / / thinkgeek . com / sf  spamassassin - sightings mailing list \n",
      "----------------------------------------------------------------------------------------------------\n",
      "7 : Subject: re : [ 3 ]  this will be our closing effort  we have aimed to speak to you on multiple possibilities and we await your response now !  your exisiting loan situation certifies you for up to a 3 . 60 % lower rate .  however , since our previous attempts to speak to  you have failed , this will be our last notice to close for you the lower rate .  please end this final step upon receiving  this notice immediately , and complete your request for information now .  apply here .  if your decision is not to make use of this final offer going here will help you to do so .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "8 : Subject: major medical breakthrough huge profit potential  major medical  breakthroughhuge profit potential  imagine yourself as part owner of the  most interesting , full service state - of - the - art medical facility , equipped  with the most sophisticated and effective scanning diagnostic tools  available today .  electron beam tomography is a  cutting - edge diagnostic technology capable of providing a  crystal - ball - like look into your medical future . this technology has been  featured on oprah , larry king , good morning america , and usa  today .  ebt scans are now covered by most health  insurance companies and hmos , causing an explosion in usership and  exceptionally high demand for this procedure .  ebt can identify heart disease years  before a treadmill test would show an abnormality and many years before a  heart attack might occur .  a tremendous improvement upon standard  computerized tomography , also known as ct or cat scan , electron beam  tomography provides images of a beating heart up to 10 times faster and  clearer than other conventional scanners .  the dramatic capabilities of this  spectacular technology should provide an extraordinary investment  opportunity for those establishing state - of - the - art outpatient clinics , in  order to provide the ebt body scan procedures to health conscious  americans . projected 10 - year return of 916 % .  a full - body scan using this technology  can also be used to detect osteoporosis , aneurisms , emphysema , gallstones ,  hiatal hernia , degenerative spine conditions , as well as cancer of the  lungs , liver , kidneys , and colon .  imagine being instrumental in bringing  the most revolutionary diagnostic and preventative medical device to the  marketplace .  $ 15 k minimum investment required .  serious inquiries only .  to recieve your free video . fill out this form .  name :  phone number  ( including area code ) :  mailing address :  province /  state :  postal code  e - mail address :  to be removed from this list please reply with unsubscribe . thank you .  http : / / xent . com / mailman / listinfo / fork\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9 : Subject: delivery status notification  - these recipients of your message have been processed by the mail server :  antoniobdantas @ zipmail . com . br ; failed ; 4 . 4 . 7 ( delivery time expired )\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 : Subject: financial maths course , part 2  vince ,  just in case , here is a draft copy of the event for you to refer to .  paul  - finmathmail . doc\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spam = df.spam\n",
    "text = df.text\n",
    "indexes = np.random.randint(low = 10, high = 5000, size = 10)\n",
    "for i in range(10):\n",
    "    print(i+1,\":\" ,text.iloc[indexes[i]])\n",
    "    print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.remove('not')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|' '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('\"', \"\", text)\n",
    "    text = re.sub(giant_url_regex, ' ', text)  #remocing the urls\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text) #removing all numbers, special chars like @,#,? etc\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject naturally irresistible corporate ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject stock trade gunslinger fanny merrill m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject unbelievable new home make easy im wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject color print special request additional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject not money get software cd software com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam  \\\n",
       "0  Subject: naturally irresistible your corporate...     1   \n",
       "1  Subject: the stock trading gunslinger  fanny i...     1   \n",
       "2  Subject: unbelievable new homes made easy  im ...     1   \n",
       "3  Subject: 4 color printing special  request add...     1   \n",
       "4  Subject: do not have money , get software cds ...     1   \n",
       "\n",
       "                                      processed_text  \n",
       "0  subject naturally irresistible corporate ident...  \n",
       "1  subject stock trade gunslinger fanny merrill m...  \n",
       "2  subject unbelievable new home make easy im wan...  \n",
       "3  subject color print special request additional...  \n",
       "4  subject not money get software cd software com...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'] = df.text.apply(lambda x: clean_text(x))   # df.review.map(clean_text) Also can be used\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency and Email Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728,)\n",
      "(5728,)\n"
     ]
    }
   ],
   "source": [
    "x = df.processed_text\n",
    "y = df['spam']\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words in the emails are :  797030\n",
      "The total UNIQUE words in the emails are :  28080\n"
     ]
    }
   ],
   "source": [
    "# finding unique words\n",
    "word_unique = []\n",
    "for i in x:\n",
    "    for j in i.split():\n",
    "        word_unique.append(j)\n",
    "unique, counts = np.unique(word_unique, return_counts=True)\n",
    "print(\"The total words in the emails are : \", len(word_unique))\n",
    "print(\"The total UNIQUE words in the emails are : \", len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Occuring Words with their frequency are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enron</td>\n",
       "      <td>13382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ect</td>\n",
       "      <td>11427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject</td>\n",
       "      <td>10201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vince</td>\n",
       "      <td>8532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hou</td>\n",
       "      <td>5577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>com</td>\n",
       "      <td>5443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>please</td>\n",
       "      <td>5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kaminski</td>\n",
       "      <td>4770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not</td>\n",
       "      <td>4575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>would</td>\n",
       "      <td>4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cc</td>\n",
       "      <td>3896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>j</td>\n",
       "      <td>3765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>thank</td>\n",
       "      <td>3725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pm</td>\n",
       "      <td>3286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>forward</td>\n",
       "      <td>3159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>time</td>\n",
       "      <td>3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meet</td>\n",
       "      <td>2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>research</td>\n",
       "      <td>2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>get</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>know</td>\n",
       "      <td>2713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1\n",
       "0      enron  13382\n",
       "1        ect  11427\n",
       "2    subject  10201\n",
       "3      vince   8532\n",
       "4        hou   5577\n",
       "5        com   5443\n",
       "6     please   5112\n",
       "7   kaminski   4770\n",
       "8        not   4575\n",
       "9      would   4426\n",
       "10        cc   3896\n",
       "11         j   3765\n",
       "12     thank   3725\n",
       "13        pm   3286\n",
       "14   forward   3159\n",
       "15      time   3119\n",
       "16      meet   2765\n",
       "17  research   2757\n",
       "18       get   2743\n",
       "19      know   2713"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting the Unique words based on their Frequency\n",
    "col = list(zip(unique, counts))\n",
    "col = sorted(col, key = lambda x: x[1],reverse=True)\n",
    "col=pd.DataFrame(col)\n",
    "print(\"Top 20 Occuring Words with their frequency are:\")\n",
    "col.iloc[:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28080.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.384259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>200.960010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13382.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1\n",
       "count  28080.000000\n",
       "mean      28.384259\n",
       "std      200.960010\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        7.000000\n",
       "max    13382.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Length :  139.14629888268158\n",
      "The max length of :  4167\n",
      "The min length of :  2\n"
     ]
    }
   ],
   "source": [
    "# finding length of emails\n",
    "length = []\n",
    "for i in x:\n",
    "    length.append(len(i.split()))\n",
    "print(\"The Average Length : \",np.mean(length))\n",
    "print(\"The max length of : \", np.max(length))\n",
    "print(\"The min length of : \", np.min(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5728.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>139.146299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>182.443202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4167.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  5728.000000\n",
       "mean    139.146299\n",
       "std     182.443202\n",
       "min       2.000000\n",
       "25%      48.000000\n",
       "50%      89.000000\n",
       "75%     167.000000\n",
       "max    4167.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = pd.DataFrame(length)\n",
    "length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 15000 )\n",
    "# tokenize and build vocab\n",
    "\n",
    "vectorizer.fit(x)\n",
    "# summarize\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.idf_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728, 15000)\n"
     ]
    }
   ],
   "source": [
    "x_tfidf = vectorizer.transform(x).toarray()\n",
    "print(x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5441, 15000)\n",
      "(287, 15000)\n",
      "(5441,)\n",
      "(287,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = tts(x_tfidf,y,test_size = 0.05)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = GaussianNB().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_predict_train = nv.predict(x_train)\n",
    "report = classification_report(y_train, nv_predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4141\n",
      "           1       0.98      1.00      0.99      1300\n",
      "\n",
      "    accuracy                           1.00      5441\n",
      "   macro avg       0.99      1.00      0.99      5441\n",
      "weighted avg       1.00      1.00      1.00      5441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       219\n",
      "           1       0.95      0.87      0.91        68\n",
      "\n",
      "    accuracy                           0.96       287\n",
      "   macro avg       0.96      0.93      0.94       287\n",
      "weighted avg       0.96      0.96      0.96       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nv_predict_test = nv.predict(x_test)\n",
    "report = classification_report(y_test, nv_predict_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = LinearSVC(class_weight='balanced', penalty='l2').fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4141\n",
      "           1       0.98      1.00      0.99      1300\n",
      "\n",
      "    accuracy                           1.00      5441\n",
      "   macro avg       0.99      1.00      0.99      5441\n",
      "weighted avg       1.00      1.00      1.00      5441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_predict_train = nv.predict(x_train)\n",
    "report_svm = classification_report(y_train, svm_predict_train)\n",
    "print(report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       219\n",
      "           1       0.95      0.87      0.91        68\n",
      "\n",
      "    accuracy                           0.96       287\n",
      "   macro avg       0.96      0.93      0.94       287\n",
      "weighted avg       0.96      0.96      0.96       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_predict_test = nv.predict(x_test)\n",
    "report = classification_report(y_test, svm_predict_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "logistic_reg_model = LogisticRegression(penalty='l2',class_weight = 'balanced',verbose=1).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4141\n",
      "           1       0.98      1.00      0.99      1300\n",
      "\n",
      "    accuracy                           0.99      5441\n",
      "   macro avg       0.99      1.00      0.99      5441\n",
      "weighted avg       0.99      0.99      0.99      5441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_predict = logistic_reg_model.predict(x_train)\n",
    "lr_model_report = classification_report(y_train, lr_model_predict)\n",
    "print(lr_model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       219\n",
      "           1       0.97      1.00      0.99        68\n",
      "\n",
      "    accuracy                           0.99       287\n",
      "   macro avg       0.99      1.00      0.99       287\n",
      "weighted avg       0.99      0.99      0.99       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_predict = logistic_reg_model.predict(x_test)\n",
    "lr_model_report = classification_report(y_test, lr_model_predict)\n",
    "print(lr_model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
